{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist classification\n",
    "\n",
    "### Exercise objectives\n",
    "- Implement a CNN architecture with convolution layers\n",
    "- Run a Neural Network on images\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "Let's imagine for a moment that you are working for the postal office (and you're in 1970 / 1980). You deal everyday with a enourmous amont of letters, and you want to automate the process of reading the numbers that have been handwritten. This task, called the _Handwriting Recognition_, has been a very complex that has been handled by Bell Labs (among other) where Yann Le Cun used to work, and where such things have been developed : \n",
    "\n",
    "![Number recognition](recognition.gif)\n",
    "\n",
    "\n",
    "The idea is that you have an image (not a video: the animation is here to present what happens with different images) as an input and you try to predict the figure on the image - it corresponds to a classification task, where the output is the class (=figure) the image belongs to, from 0 to 9.\n",
    "\n",
    "This task used to be quite complex back in the time, and still is a benchmark on which a lot of people work. For this reason, the MNIST (for *Modified ou Mixed National Institute of Standards and Technology*) dataset has been created: it corresponds to digit images, from 0 to 9. \n",
    "\n",
    "You goal in this notebook is to build a Convolution Neural Network that can work on such images and predict the corresponding class of each digit image. Keep in mind that this CNN will make you classify hand-written digits, which was a very complex task till the 90's. \n",
    "\n",
    "## The data\n",
    "\n",
    "Keras provides multiple datasets within the Python package. You can load it with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "df = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T07:22:19.145710Z",
     "start_time": "2021-04-29T07:22:14.174055Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "data = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "(X_train, y_train), (X_test, y_test) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape == (60000, 28, 28)\n",
    "assert X_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and configure the data generator\n",
    "datagen = ImageDataGenerator(data)\n",
    "datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ Let's look at some of the data. \n",
    "\n",
    "Select some of the values of the train set and plot them thanks to the `imshow` function from matplotlib with `cmap` set to `gray`(otherwise, the displayed colors are just some arrangement Matplotlib does, which does not exist in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23fc9eabfa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23fc9f93790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that neural networks converge faster when the input data are somehow normalized? It goes similarly for input images. \n",
    "\n",
    "❓ Question ❓ As a first preprocessing step, you should normalize your data. For images, it simply implies to divide your input data by the maximal value, i.e. 255. Don't forget to do it on your train and test data.\n",
    "\n",
    "(N.B.: you can also centered your data, by substracting 0.5 but it is not mandatory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T07:22:19.731522Z",
     "start_time": "2021-04-29T07:22:19.148711Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train_normalise = X_train/255.0\n",
    "X_test_normalise = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalise[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ What is the shape of your images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23fc9ff65b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "plt.imshow(X_train_normalise[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that you have 60.000 training images, all of size (28, 28). However, Keras needs images whose last dimension is the number of channels, which is missing here.\n",
    "\n",
    "❓ Question ❓ Use the `expand_dims` to add one dimension at the end of the training and test data. Then, print the shape of X_train and X_test that should respectively be (60000, 28, 28, 1) and (10000, 28, 28, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60 000 données  : 60000\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "print(\" 60 000 données  :\", N)\n",
    "X_train = np.reshape(X_train, (N, 28,28,1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partie A \n",
    "X_train = np.reshape(X_train, (N, 28,28,1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],28,28,1))\n",
    "#normalisation\n",
    "X_train_n = X_train/255\n",
    "X_test_n = X_test/255\n",
    "#On a 10 cathegorie des nombres\n",
    "y_train_ca = to_categorical(y_train, num_classes=10)\n",
    "y_test_ca = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (60000, 28, 28, 32)       320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (60000, 28, 28, 16)       4624      \n",
      "                                                                 \n",
      " flatten (Flatten)           (60000, 12544)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (60000, 10)               125450    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,394\n",
      "Trainable params: 130,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Partie B Reseau de neurones\n",
    "modele = Sequential()\n",
    "#Premier couche de convolution: 32 neurones(nbre de filtre), convolution 3*3(precise la taille du conv), activation \n",
    "modele.add(Conv2D(32, kernel_size=3, padding= 'same', activation='relu'))\n",
    "#Deuxieme couche de convolution 16 neurones\n",
    "modele.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\n",
    "#Aplatissage\n",
    "modele.add(Flatten())\n",
    "#couche de sortie : 10 neurones\n",
    "modele.add(Dense(10, activation='softmax'))\n",
    "#descente de gradiant: type d'erreur \"loss\" , methede de gradiant tre efficace \"adam\"\n",
    "modele.compile(optimizer= 'adam',loss='categorical_crossentrepy', metrics=['categorical_crossentropy'])\n",
    "\n",
    "#modele.compile(optimizer='adam',loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "modele.build(input_shape = (N, 28, 28, 1))\n",
    "modele.summary()\n",
    "#On a totale 130.394 poids à calcules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#calcul des poids: On va lance en utilisant une descente de gradiant par lot de taille 32\n",
    "modele.fit(X_train_n, y_train_ca, batch_size=32, epochs=5)\n",
    "#Partie C : Evalue la qualite des reseau obtenu n'on pas sur nos données d'apprentissage plutot donne independantes\n",
    "score = modele.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T07:22:20.063475Z",
     "start_time": "2021-04-29T07:22:20.052948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_normalise          : (60000, 28, 28)\n",
      "X_train_normalise_elargi   : (60000, 28, 28, 1)\n",
      "X_test_normalise           : (10000, 28, 28)\n",
      "X_test_normalise_elargi    : (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_normalise_elargi = np.expand_dims(X_train_normalise,axis=3)\n",
    "X_test_normalise_elargi = np.expand_dims(X_test_normalise,axis=3)\n",
    "print(\"X_train_normalise          :\",X_train_normalise.shape)\n",
    "print(\"X_train_normalise_elargi   :\",X_train_normalise_elargi.shape)\n",
    "print(\"X_test_normalise           :\",X_test_normalise.shape)\n",
    "print(\"X_test_normalise_elargi    :\",X_test_normalise_elargi.shape)\n",
    "#60000, 28, 28, 1 : code avec 28*28 avec un seul canal d'entre 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last thing to do to prepare your data is to convert your labels to one-hot encoded categories.\n",
    "\n",
    "❓ Question ❓ Use `to_categorical` to transform your labels. Store the results in `y_train_cat` and `y_test_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T07:22:20.845683Z",
     "start_time": "2021-04-29T07:22:20.840921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_cat : (60000, 10)\n",
      "y_test_cat : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=None, dtype='float32')\n",
    "y_test_cat = tf.keras.utils.to_categorical( y_test, num_classes=None, dtype='float32')\n",
    "#On a 10 cathegorie des nombres\n",
    "print(\"y_train_cat :\",y_train_cat.shape)\n",
    "print(\"y_test_cat :\",y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now ready to be used.\n",
    "\n",
    "## The Convolutional Neural Network _aka_ CNN\n",
    "\n",
    "Now, build a Convolutional Neural Network. \n",
    "\n",
    "❓ Question ❓ Based on the course, build a neural network that has:\n",
    "- a `Conv2D` layer with 8 filters, each of size (4, 4), with an input shape suitable for your task, the relu activation function, and padding='same' so as to \n",
    "- a `MaxPool2D` layer with a pool_size of (2, 2)\n",
    "- a second `Conv2D` layer with 16 filters, each of size (3, 3), and the relu activation function\n",
    "- a second `MaxPool2D` layer with a pool_size of (2, 2)\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the relu activation function\n",
    "- a last layer that is suited for your task\n",
    "\n",
    "In the function, do not forget to include the compilation of the model, which optimizes the `categorical_crossentropy` with the adam optimizer - and the accuracy should be among the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ How many trainable parameters are there in your model?\n",
    "- Compute them with `model.summary()` first\n",
    "- Recompute them manually layer per layer then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T07:22:24.419923Z",
     "start_time": "2021-04-29T07:22:24.415553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 25, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                4010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,314\n",
      "Trainable params: 5,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        #First convolution & max-pooling\n",
    "        layers.Conv2D(8,(4,4), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Second convolution & max-pooling\n",
    "        layers.Conv2D(16,(3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        #Sortie\n",
    "        #tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        #10: nbre de categaries\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "       \n",
    "        ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_crossentropy'])\n",
    "\n",
    "    \"\"\"model.compile(optimizer='adam',\n",
    "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\"\"\"\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = initialize_model()\n",
    "model.build(input_shape = (None, 28, 28, 1))\n",
    "      \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 24s 12ms/step - loss: 1.3781 - categorical_crossentropy: 1.3781\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4023 - categorical_crossentropy: 0.4023\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3283 - categorical_crossentropy: 0.3283\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2822 - categorical_crossentropy: 0.2822\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2442 - categorical_crossentropy: 0.2442\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_normalise_elargi, y_train_cat,\n",
    "                    batch_size=32,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Question ❓ Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a validation set and an early stopping criterion. \n",
    "- Limit at 5 epoch max in this challenge (just to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T07:22:50.611532Z",
     "start_time": "2021-04-29T07:22:50.608695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1313/1313 [==============================] - 20s 15ms/step - loss: 0.0100 - MAE: 0.0227 - MSE: 0.0100 - val_loss: 0.0091 - val_MAE: 0.0205 - val_MSE: 0.0091\n",
      "Epoch 2/5\n",
      "1313/1313 [==============================] - 19s 15ms/step - loss: 0.0092 - MAE: 0.0210 - MSE: 0.0092 - val_loss: 0.0087 - val_MAE: 0.0205 - val_MSE: 0.0087\n",
      "Epoch 3/5\n",
      "1313/1313 [==============================] - 20s 15ms/step - loss: 0.0086 - MAE: 0.0197 - MSE: 0.0086 - val_loss: 0.0084 - val_MAE: 0.0183 - val_MSE: 0.0084\n",
      "Epoch 4/5\n",
      "1313/1313 [==============================] - 19s 14ms/step - loss: 0.0080 - MAE: 0.0184 - MSE: 0.0080 - val_loss: 0.0079 - val_MAE: 0.0178 - val_MSE: 0.0079\n",
      "Epoch 5/5\n",
      "1313/1313 [==============================] - 18s 14ms/step - loss: 0.0076 - MAE: 0.0174 - MSE: 0.0076 - val_loss: 0.0074 - val_MAE: 0.0170 - val_MSE: 0.0074\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0068 - MAE: 0.0160 - MSE: 0.0068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.callbacks.EarlyStopping at 0x23fcbb9f0a0>, 0.016033370047807693)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def trouve_ovorfiting():\n",
    "    ovorfiting = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights= True)\n",
    "    model.compile(optimizer='adam', loss='MSE', metrics=['MAE','MSE'])\n",
    "    history = model.fit(X_train_normalise_elargi, y_train_cat, batch_size=32, epochs=5, validation_split=0.3)\n",
    "    resultat = model.evaluate(X_test_normalise_elargi, y_test_cat)\n",
    "    \n",
    "    return ovorfiting, resultat[1]\n",
    "trouve_ovorfiting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably see that the model converges within few epochs. The reason is that there are as many weight update as there are batches within each epoch. For instance, if you batch_size is of 32, you have 60.000/32 = 1875 updates.\n",
    "\n",
    "\n",
    "❓ Question ❓ What is your accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.21320490539073944\n",
      "Test accuracy 0.21320490539073944\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_normalise, y_test_cat, verbose=0)\n",
    "print('Test loss', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.02315095e-07, 7.69736497e-10, 9.29699624e-08, ...,\n",
       "        9.99879003e-01, 1.37422461e-07, 1.00106583e-04],\n",
       "       [1.15444076e-04, 2.90879630e-03, 9.95099127e-01, ...,\n",
       "        1.13115095e-12, 5.95801801e-04, 1.65271113e-11],\n",
       "       [1.76150439e-04, 9.90534544e-01, 2.13021645e-03, ...,\n",
       "        1.62673125e-03, 1.15307921e-03, 4.88258433e-04],\n",
       "       ...,\n",
       "       [1.88517845e-06, 7.30114834e-07, 2.59443041e-04, ...,\n",
       "        1.40420684e-06, 2.03364289e-05, 9.07011000e-08],\n",
       "       [2.75999724e-07, 2.00204553e-07, 2.04155276e-06, ...,\n",
       "        1.14236225e-03, 6.17897138e-03, 4.09053043e-02],\n",
       "       [1.36276154e-04, 7.51221596e-05, 1.98914622e-05, ...,\n",
       "        1.05246407e-04, 8.78595486e-02, 2.70879507e-04]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_normalise_elargi, verbose=1)\n",
    "predictions[0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should be already impressed by your skills! You solved what was a very hard problem 30 years ago with your CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 Congratulation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
